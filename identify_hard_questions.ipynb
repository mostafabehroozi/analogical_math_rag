{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --- Our project's modules ---\n",
    "from config import CONFIG, setup_directories\n",
    "from src.utils import setup_logger, load_json, save_json\n",
    "from src.api_manager import GeminiAPIManager\n",
    "from src.orchestration import run_pipeline_for_single_query\n",
    "from src.hf_sync import sync_workspace_to_hub\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- 1. Initial Setup ---\n",
    "# Create the directory structure (logs, results, etc.)\n",
    "setup_directories()\n",
    "\n",
    "# Initialize the main logger for this notebook\n",
    "logger = setup_logger('hard_question_identifier', log_dir=CONFIG['LOGS_DIR'])\n",
    "\n",
    "# --- 2. Load Configuration & Initialize API Manager ---\n",
    "# Extract the dedicated config for this task for easier access\n",
    "ID_CONFIG = CONFIG['HARD_QUESTION_IDENTIFICATION_CONFIG']\n",
    "\n",
    "logger.info(\"--- Hard Question Identification Notebook Started ---\")\n",
    "logger.info(f\"Loaded configuration: {json.dumps(ID_CONFIG, indent=2)}\")\n",
    "\n",
    "# Initialize the Gemini API Manager\n",
    "try:\n",
    "    gemini_manager = GeminiAPIManager(\n",
    "        api_keys=CONFIG['GEMINI_API_KEYS'],\n",
    "        model_quotas=CONFIG['GEMINI_MODEL_QUOTAS'],\n",
    "        global_delay_seconds=CONFIG['GLOBAL_API_CALL_DELAY_SECONDS']\n",
    "    )\n",
    "    logger.info(\"GeminiAPIManager initialized successfully.\")\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Failed to initialize GeminiAPIManager. Aborting. Error: {e}\", exc_info=True)\n",
    "    gemini_manager = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Load Dataset and Source Questions ---\n",
    "target_indices = []\n",
    "all_questions = []\n",
    "all_ground_truths = []\n",
    "\n",
    "if gemini_manager:\n",
    "    try:\n",
    "        # Load the main dataset which serves as the source for all questions\n",
    "        main_ds = load_dataset(CONFIG['EXEMPLAR_CORPUS_NAME'], split='train')\n",
    "        all_questions = main_ds['problem']\n",
    "        all_ground_truths = main_ds['solution']\n",
    "        logger.info(f\"Successfully loaded the main dataset with {len(all_questions)} questions.\")\n",
    "\n",
    "        # --- Flexible Question Sourcing Logic ---\n",
    "        # Check if a specific list of indices is provided in the config\n",
    "        if ID_CONFIG.get(\"TARGET_INDICES_FILE_PATH\") and os.path.exists(ID_CONFIG[\"TARGET_INDICES_FILE_PATH\"]):\n",
    "            target_indices = load_json(ID_CONFIG[\"TARGET_INDICES_FILE_PATH\"])\n",
    "            logger.info(f\"Loaded {len(target_indices)} target indices from the specified file: {ID_CONFIG['TARGET_INDICES_FILE_PATH']}\")\n",
    "        \n",
    "        # If not, generate a random sample of indices\n",
    "        else:\n",
    "            num_samples = ID_CONFIG.get(\"NUM_RANDOM_SAMPLES\", 100)\n",
    "            if num_samples > len(all_questions):\n",
    "                logger.warning(f\"Number of samples ({num_samples}) is larger than the dataset size ({len(all_questions)}). Using all questions.\")\n",
    "                target_indices = list(range(len(all_questions)))\n",
    "            else:\n",
    "                target_indices = random.sample(range(len(all_questions)), num_samples)\n",
    "                logger.info(f\"Generated a random sample of {len(target_indices)} indices.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"An error occurred during data loading or question sourcing. Aborting. Error: {e}\", exc_info=True)\n",
    "        target_indices = []\n",
    "\n",
    "print(f\"Prepared to process {len(target_indices)} questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b9830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Run the Identification Pipeline ---\n",
    "run_logs = []\n",
    "log_file_path = os.path.join(CONFIG['RESULTS_DIR'], \"hard_question_identification_log.json\")\n",
    "\n",
    "if gemini_manager and target_indices:\n",
    "    # --- Setup for non-RAG execution ---\n",
    "    # Create a specific configuration for this run\n",
    "    run_config = CONFIG.copy()\n",
    "    run_config.update({\n",
    "        \"USE_RETRIEVAL\": False, # Crucial: We are testing the model's standalone ability\n",
    "        \"ONLINE_EVALUATION_ENABLED\": True, # Crucial: We need real-time results\n",
    "        \"STOP_ON_FIRST_SUCCESS\": False # We need to see if it fails all attempts\n",
    "    })\n",
    "    \n",
    "    # Check for existing logs to resume\n",
    "    existing_logs = load_json(log_file_path)\n",
    "    if existing_logs:\n",
    "        run_logs = existing_logs\n",
    "        completed_indices = {log.get('target_query_original_hard_list_idx') for log in run_logs}\n",
    "        logger.info(f\"Resuming run. Loaded {len(run_logs)} existing results. {len(completed_indices)} questions already processed.\")\n",
    "    else:\n",
    "        completed_indices = set()\n",
    "\n",
    "    # Filter out already processed questions\n",
    "    indices_to_process = [idx for idx in target_indices if idx not in completed_indices]\n",
    "    \n",
    "    logger.info(f\"Starting pipeline for {len(indices_to_process)} new questions.\")\n",
    "    \n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    for original_idx in tqdm(indices_to_process, desc=\"Identifying Hard Questions\"):\n",
    "        query_text = all_questions[original_idx]\n",
    "        ground_truth_text = all_ground_truths[original_idx]\n",
    "\n",
    "        # Run the pipeline for the single query\n",
    "        single_run_log = run_pipeline_for_single_query(\n",
    "            hard_list_idx=original_idx,\n",
    "            target_query=query_text,\n",
    "            ground_truth=ground_truth_text,\n",
    "            config=run_config,\n",
    "            embedding_model=None, # Not needed as retrieval is off\n",
    "            exemplar_data={}, # Not needed as retrieval is off\n",
    "            gemini_manager=gemini_manager\n",
    "        )\n",
    "        \n",
    "        run_logs.append(single_run_log)\n",
    "        \n",
    "        # Save progress after each question\n",
    "        save_json(run_logs, log_file_path)\n",
    "    \n",
    "    logger.info(\"Pipeline execution complete for all selected questions.\")\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping pipeline execution because the API manager or target indices are not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Analyze Results to Identify Hard Questions ---\n",
    "hard_question_indices = []\n",
    "\n",
    "if run_logs:\n",
    "    logger.info(\"Analyzing the results to identify hard questions...\")\n",
    "    \n",
    "    # Filter out any runs that failed due to technical issues (not model inability)\n",
    "    evaluable_logs = [log for log in run_logs if not log['pipeline_status'].startswith('UN-EVALUABLE')]\n",
    "    \n",
    "    num_unevaluable = len(run_logs) - len(evaluable_logs)\n",
    "    if num_unevaluable > 0:\n",
    "        logger.warning(f\"{num_unevaluable} questions were excluded from analysis because they were un-evaluable.\")\n",
    "        \n",
    "    for log in evaluable_logs:\n",
    "        # Check the online evaluation results for any correct answer\n",
    "        evaluation_results = log.get(\"online_evaluation_results\", [])\n",
    "        was_solved = any(result.get(\"is_correct\") for result in evaluation_results)\n",
    "        \n",
    "        # If it was never solved, it's a hard question\n",
    "        if not was_solved:\n",
    "            hard_question_indices.append(log[\"target_query_original_hard_list_idx\"])\n",
    "            \n",
    "    logger.info(f\"Analysis complete. Found {len(hard_question_indices)} hard questions out of {len(evaluable_logs)} evaluable questions.\")\n",
    "    print(f\"\\n--- Analysis Summary ---\")\n",
    "    print(f\"Total Questions Processed: {len(run_logs)}\")\n",
    "    print(f\"Evaluable Questions: {len(evaluable_logs)}\")\n",
    "    print(f\"Identified Hard Questions: {len(hard_question_indices)}\")\n",
    "    \n",
    "else:\n",
    "    logger.warning(\"No run logs were found to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Save the Final List and Synchronize ---\n",
    "if hard_question_indices:\n",
    "    output_path = ID_CONFIG['HARD_QUESTIONS_OUTPUT_PATH']\n",
    "    logger.info(f\"Saving the list of {len(hard_question_indices)} hard question indices to: {output_path}\")\n",
    "    \n",
    "    # Save the list to the specified file\n",
    "    save_json(hard_question_indices, output_path)\n",
    "    \n",
    "    print(f\"\\nSuccessfully saved the hard question indices to '{output_path}'.\")\n",
    "\n",
    "else:\n",
    "    logger.info(\"No hard questions were identified, so no output file was saved.\")\n",
    "\n",
    "# --- Final synchronization to save all results and logs ---\n",
    "if CONFIG.get(\"PERSIST_RESULTS_ONLINE\"):\n",
    "    logger.info(\"Performing final synchronization to Hugging Face Hub...\")\n",
    "    sync_workspace_to_hub(CONFIG)\n",
    "    logger.info(\"Final sync complete. All results from this run are saved online.\")\n",
    "\n",
    "logger.info(\"--- Hard Question Identification Notebook Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad0437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Check API Keys ---\n",
    "from src.utils import check_api_keys\n",
    "\n",
    "if gemini_manager:\n",
    "    check_api_keys(CONFIG)\n",
    "else:\n",
    "    logger.warning(\"Skipping API key check because GeminiAPIManager failed to initialize.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e86597",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install datasets\n",
    "!pip install huggingface_hub\n",
    "!pip install openai\n",
    "!pip install typing-extensions  # Often a useful dependency for TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download, HfFolder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Our project's modules ---\n",
    "from config import CONFIG, setup_directories\n",
    "from src.utils import setup_logger, load_json\n",
    "# MODIFIED: Import both API manager classes\n",
    "from src.api_manager import GeminiAPIManager, AvalAIAPIManager\n",
    "from src.orchestration import run_experiments\n",
    "from src.evaluation import analyze_experiment_logs\n",
    "from src.hf_sync import initialize_workspace, sync_workspace_to_hub\n",
    "\n",
    "# --- Initial Setup ---\n",
    "# 1. Create the directory structure (logs, results, etc.)\n",
    "setup_directories()\n",
    "\n",
    "# 2. Initialize the main logger for the notebook\n",
    "logger = setup_logger('main_orchestrator', log_dir=CONFIG['LOGS_DIR'])\n",
    "\n",
    "# --- Hugging Face Hub Workspace Initialization ---\n",
    "if CONFIG.get('PERSIST_RESULTS_ONLINE'):\n",
    "    if CONFIG.get(\"HF_SYNC_TOKEN\"):\n",
    "        logger.info(\"HF_SYNC_TOKEN found in config. Initializing workspace from Hugging Face Hub.\")\n",
    "        initialize_workspace(CONFIG)\n",
    "    else:\n",
    "        logger.warning(\"PERSIST_RESULTS_ONLINE is True, but no HF_SYNC_TOKEN was found in config.py.\")\n",
    "\n",
    "# --- API Manager Factory ---\n",
    "# This block dynamically instantiates the correct API manager based on the config.\n",
    "api_manager = None\n",
    "provider = CONFIG.get(\"API_PROVIDER\", \"gemini\").lower()\n",
    "logger.info(f\"Selected API Provider from config: '{provider}'\")\n",
    "\n",
    "if provider == \"gemini\":\n",
    "    try:\n",
    "        api_manager = GeminiAPIManager(\n",
    "            api_keys=CONFIG['GEMINI_API_KEYS'],\n",
    "            model_quotas=CONFIG['GEMINI_MODEL_QUOTAS'],\n",
    "            global_delay_seconds=CONFIG['GLOBAL_API_CALL_DELAY_SECONDS'],\n",
    "            config=CONFIG\n",
    "        )\n",
    "        logger.info(\"GeminiAPIManager initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to initialize GeminiAPIManager. Aborting. Error: {e}\", exc_info=True)\n",
    "\n",
    "elif provider == \"avalai\":\n",
    "    try:\n",
    "        api_manager = AvalAIAPIManager(\n",
    "            api_key=CONFIG['AVALAI_API_KEY'],\n",
    "            base_url=CONFIG['AVALAI_BASE_URL'],\n",
    "            model_quotas=CONFIG['AVALAI_MODEL_QUOTAS'],\n",
    "            # MODIFIED: Pass the global delay to the constructor\n",
    "            global_delay_seconds=CONFIG['GLOBAL_API_CALL_DELAY_SECONDS'],\n",
    "            config=CONFIG\n",
    "        )\n",
    "        logger.info(\"AvalAIAPIManager initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to initialize AvalAIAPIManager. Aborting. Error: {e}\", exc_info=True)\n",
    "\n",
    "logger.info(\"Notebook execution started. Directories, logger, and API Manager are set up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell handles loading all necessary assets.\n",
    "\n",
    "# --- 1. Load Embedding Model ---\n",
    "try:\n",
    "    embedding_model = SentenceTransformer(CONFIG['EMBEDDING_MODEL_PATH'])\n",
    "    logger.info(f\"Successfully loaded SentenceTransformer model from: {CONFIG['EMBEDDING_MODEL_PATH']}\")\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Failed to load embedding model. Aborting. Error: {e}\", exc_info=True)\n",
    "    embedding_model = None\n",
    "\n",
    "# --- 2. Load Full Exemplar Corpus and Embeddings ---\n",
    "exemplar_data = {}\n",
    "if embedding_model:\n",
    "    try:\n",
    "        logger.info(f\"Loading exemplar corpus: {CONFIG['EXEMPLAR_CORPUS_NAME']}\")\n",
    "        exemplar_ds = load_dataset(CONFIG['EXEMPLAR_CORPUS_NAME'], split='train')\n",
    "        \n",
    "        logger.info(f\"Loading pre-computed embeddings from: {CONFIG['EMBEDDED_EXEMPLAR_CORPUS_QUESTIONS_PATH']}\")\n",
    "        exemplar_embeddings = np.load(CONFIG['EMBEDDED_EXEMPLAR_CORPUS_QUESTIONS_PATH'])\n",
    "        \n",
    "        exemplar_data = {\n",
    "            'questions': exemplar_ds['problem'],\n",
    "            'solutions': exemplar_ds['solution'],\n",
    "            'embeddings': exemplar_embeddings\n",
    "        }\n",
    "        logger.info(f\"Successfully loaded {len(exemplar_data['questions'])} exemplars and their embeddings.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to load exemplar data or embeddings. Aborting. Error: {e}\", exc_info=True)\n",
    "\n",
    "# --- 3. Load \"Hard Questions\" (Target Queries) using Indices ---\n",
    "hard_questions_list = []\n",
    "hard_questions_ground_truths = []\n",
    "if exemplar_data:\n",
    "    try:\n",
    "        hard_question_indices = load_json(CONFIG['HARD_QUESTIONS_INDICES_PATH'])\n",
    "        if hard_question_indices:\n",
    "            hard_questions_list = [exemplar_data['questions'][i] for i in hard_question_indices]\n",
    "            hard_questions_ground_truths = [exemplar_data['solutions'][i] for i in hard_question_indices]\n",
    "            logger.info(f\"Successfully loaded {len(hard_questions_list)} hard questions and their ground truths using indices.\")\n",
    "        else:\n",
    "            logger.critical(f\"Hard questions index file not found or empty at {CONFIG['HARD_QUESTIONS_INDICES_PATH']}. Aborting.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"An error occurred loading hard questions. Aborting. Error: {e}\", exc_info=True)\n",
    "\n",
    "# --- 4. Final Check ---\n",
    "if not all([api_manager, embedding_model, exemplar_data, hard_questions_list]):\n",
    "    logger.critical(\"One or more critical assets failed to load. Please check the logs above. Halting execution.\")\n",
    "else:\n",
    "    logger.info(\"All assets loaded successfully. Ready to run experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the control panel for your research. Define different experiment\n",
    "# configurations by creating dictionaries that override the default CONFIG.\n",
    "\n",
    "experiment_configurations = [\n",
    "    {\n",
    "        \"experiment_name\": \"RAG_Only_Pass@3\",\n",
    "        \"USE_RETRIEVAL\": True,\n",
    "        \"APPLY_STANDARDIZATION\": False,\n",
    "        \"APPLY_TRANSFORMATION\": False,\n",
    "        \"APPLY_MERGING\": False,\n",
    "        \"TOP_N_CANDIDATES_RETRIEVAL\": 1,\n",
    "        \"N_PASS_ATTEMPTS\": 3,\n",
    "    },\n",
    "    {\n",
    "        \"experiment_name\": \"No_RAG_Baseline_Pass@3\",\n",
    "        \"USE_RETRIEVAL\": False, # Experiment without RAG\n",
    "        \"N_PASS_ATTEMPTS\": 3,\n",
    "    },\n",
    "    {\n",
    "        \"experiment_name\": \"Full_Pipeline_K3_Pass@1\",\n",
    "        \"USE_RETRIEVAL\": True,\n",
    "        \"APPLY_STANDARDIZATION\": True,\n",
    "        \"APPLY_TRANSFORMATION\": True,\n",
    "        \"APPLY_MERGING\": True,\n",
    "        \"TOP_N_CANDIDATES_RETRIEVAL\": 3,\n",
    "        \"TARGET_ADAPTED_SAMPLES_MERGING\": 1,\n",
    "        \"N_PASS_ATTEMPTS\": 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "logger.info(f\"Defined {len(experiment_configurations)} experiments to run.\")\n",
    "print(\"Experiments to run:\")\n",
    "for exp in experiment_configurations:\n",
    "    print(f\"- {exp['experiment_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084fb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell executes all defined experiments.\n",
    "# It will save logs for each experiment, allowing you to resume if the run is interrupted.\n",
    "\n",
    "all_experiment_logs = {}\n",
    "if 'api_manager' in locals() and api_manager is not None:\n",
    "    logger.info(\"Starting the main experiment execution loop.\")\n",
    "    \n",
    "    all_experiment_logs = run_experiments(\n",
    "        experiment_configs=experiment_configurations,\n",
    "        global_config=CONFIG,\n",
    "        hard_questions=hard_questions_list,\n",
    "        embedding_model=embedding_model,\n",
    "        exemplar_data=exemplar_data,\n",
    "        api_manager=api_manager\n",
    "    )\n",
    "    \n",
    "    logger.info(\"All experiments have been processed.\")\n",
    "    \n",
    "    # --- Final synchronization to save all run logs ---\n",
    "    logger.info(\"Performing final synchronization of run logs to Hugging Face Hub.\")\n",
    "    sync_workspace_to_hub(CONFIG)\n",
    "    logger.info(\"Final sync of run logs complete.\")\n",
    "\n",
    "else:\n",
    "    logger.error(\"API Manager not initialized. Cannot run experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af26005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell attempts to retry any generation pipelines that failed during the\n",
    "# main experiment run due to API errors or other issues.\n",
    "\n",
    "from src.error_handler import retry_failed_generation_pipelines\n",
    "\n",
    "if all_experiment_logs:\n",
    "    logger.info(\"Starting the retry process for failed generation pipelines.\")\n",
    "    \n",
    "    # This function will modify the logs in 'all_experiment_logs' in place\n",
    "    # and also update the JSON files on disk.\n",
    "    retry_failed_generation_pipelines(\n",
    "        all_experiments_logs=all_experiment_logs,\n",
    "        global_config=CONFIG,\n",
    "        hard_questions=hard_questions_list,\n",
    "        embedding_model=embedding_model,\n",
    "        exemplar_data=exemplar_data,\n",
    "        api_manager=api_manager\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Generation retry process complete. The experiment logs have been updated.\")\n",
    "    \n",
    "    # --- Final sync after generation retries ---\n",
    "    logger.info(\"Performing final synchronization after generation retries.\")\n",
    "    sync_workspace_to_hub(CONFIG)\n",
    "    logger.info(\"Final sync complete.\")\n",
    "\n",
    "else:\n",
    "    logger.warning(\"No experiment logs were loaded. Skipping generation retry process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac005a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes the logs generated by the experiments and runs the\n",
    "# LLM-based evaluation to produce the final Pass@K summary.\n",
    "\n",
    "if all_experiment_logs:\n",
    "    logger.info(\"Starting analysis of experiment results.\")\n",
    "    \n",
    "    summary_df = analyze_experiment_logs(\n",
    "        all_experiments_logs=all_experiments_logs,\n",
    "        ground_truths=hard_questions_ground_truths,\n",
    "        api_manager=api_manager,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    summary_path = os.path.join(CONFIG['RESULTS_DIR'], \"final_experiment_summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    logger.info(f\"Analysis complete. Summary saved to {summary_path}\")\n",
    "    \n",
    "    print(\"\\n--- Experiment Summary ---\")\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    display(summary_df)\n",
    "\n",
    "    # --- Final sync to save evaluation results ---\n",
    "    logger.info(\"Performing final synchronization of evaluation results to Hugging Face Hub.\")\n",
    "    sync_workspace_to_hub(CONFIG)\n",
    "    logger.info(\"Final sync complete. All results are saved online.\")\n",
    "\n",
    "else:\n",
    "    logger.warning(\"No experiment logs were generated or loaded. Skipping analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21075dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell attempts to retry only the evaluations that failed due to API errors\n",
    "# or parsing issues. It updates the detailed evaluation JSON files in place.\n",
    "\n",
    "from src.error_handler import retry_failed_evaluations\n",
    "\n",
    "if all_experiment_logs:\n",
    "    logger.info(\"Starting the retry process for failed evaluations.\")\n",
    "    \n",
    "    # This function modifies the detailed_eval.json files on disk.\n",
    "    # It does not return anything.\n",
    "    retry_failed_evaluations(\n",
    "        all_experiments_logs=all_experiments_logs,\n",
    "        ground_truths=hard_questions_ground_truths,\n",
    "        api_manager=api_manager,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Evaluation retry process complete. To see the updated summary, re-run the analysis cell below.\")\n",
    "    \n",
    "    # --- Final sync after evaluation retries ---\n",
    "    logger.info(\"Performing final synchronization after retries.\")\n",
    "    sync_workspace_to_hub(CONFIG)\n",
    "    logger.info(\"Final sync complete.\")\n",
    "\n",
    "else:\n",
    "    logger.warning(\"No experiment logs were loaded. Skipping evaluation retry process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell re-analyzes the experiment logs after the evaluation retry process.\n",
    "# This provides the final, updated summary reflecting the successful retries.\n",
    "\n",
    "if all_experiment_logs:\n",
    "    logger.info(\"Recalculating the experiment summary after retries.\")\n",
    "    \n",
    "    # analyze_experiment_logs is smart enough to use the updated evaluation files.\n",
    "    updated_summary_df = analyze_experiment_logs(\n",
    "        all_experiments_logs=all_experiment_logs,\n",
    "        ground_truths=hard_questions_ground_truths,\n",
    "        api_manager=api_manager,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    updated_summary_path = os.path.join(CONFIG['RESULTS_DIR'], \"final_experiment_summary_after_retry.csv\")\n",
    "    updated_summary_df.to_csv(updated_summary_path, index=False)\n",
    "    logger.info(f\"Analysis after retry complete. New summary saved to {updated_summary_path}\")\n",
    "    \n",
    "    print(\"\\n--- Experiment Summary After Retries ---\")\n",
    "    display(updated_summary_df)\n",
    "\n",
    "    # --- Final sync of updated summary ---\n",
    "    sync_workspace_to_hub(CONFIG)\n",
    "\n",
    "else:\n",
    "    logger.warning(\"No experiment logs loaded, cannot recalculate summary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell generates a detailed report of all errors that occurred\n",
    "# during both the generation and evaluation processes.\n",
    "\n",
    "from src.error_handler import generate_error_report\n",
    "\n",
    "if all_experiment_logs:\n",
    "    logger.info(\"Generating a detailed report of all errors from the logs.\")\n",
    "    \n",
    "    error_df = generate_error_report(\n",
    "        all_experiments_logs=all_experiments_logs,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    if not error_df.empty:\n",
    "        print(\"\\n--- Comprehensive Error Report ---\")\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', 1000)\n",
    "        pd.set_option('display.max_rows', 100)\n",
    "        display(error_df)\n",
    "        \n",
    "        error_report_path = os.path.join(CONFIG['RESULTS_DIR'], \"comprehensive_error_report.csv\")\n",
    "        error_df.to_csv(error_report_path, index=False)\n",
    "        logger.info(f\"Comprehensive error report saved to {error_report_path}\")\n",
    "        \n",
    "        # Sync the new report\n",
    "        sync_workspace_to_hub(CONFIG)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n--- No Errors Found ---\")\n",
    "        logger.info(\"The error report is empty because no failures were logged in any phase.\")\n",
    "else:\n",
    "    logger.warning(\"No experiment logs loaded, cannot generate an error report.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e86597",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install datasets\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download, HfFolder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Our project's modules ---\n",
    "from config import CONFIG, setup_directories\n",
    "from src.utils import setup_logger, load_json\n",
    "from src.api_manager import GeminiAPIManager\n",
    "from src.orchestration import run_experiments\n",
    "from src.evaluation import analyze_experiment_logs\n",
    "# --- NEW: Import for HF synchronization ---\n",
    "from src.hf_sync import initialize_workspace\n",
    "\n",
    "# --- Initial Setup ---\n",
    "# 1. Create the directory structure (logs, results, etc.)\n",
    "setup_directories()\n",
    "\n",
    "# 2. Initialize the main logger for the notebook\n",
    "logger = setup_logger('main_orchestrator', log_dir=CONFIG['LOGS_DIR'])\n",
    "\n",
    "# --- MODIFIED: Hugging Face Hub Workspace Initialization from Config ---\n",
    "if CONFIG.get('PERSIST_RESULTS_ONLINE'):\n",
    "    # The token is now read directly from the CONFIG dictionary.\n",
    "    # Make sure you have set it in your config.py file.\n",
    "    if CONFIG.get(\"HF_SYNC_TOKEN\"):\n",
    "        logger.info(\"HF_SYNC_TOKEN found in config. Initializing workspace from Hugging Face Hub.\")\n",
    "        initialize_workspace(CONFIG)\n",
    "    else:\n",
    "        logger.warning(\"PERSIST_RESULTS_ONLINE is True, but no HF_SYNC_TOKEN was found in config.py.\")\n",
    "        logger.warning(\"Online persistence will fail. Please add your token to config.py to enable it.\")\n",
    "\n",
    "logger.info(\"Notebook execution started. Directories and logger are set up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell handles loading all necessary assets. If any part fails,\n",
    "# the execution should stop here.\n",
    "\n",
    "# --- 1. Load Embedding Model ---\n",
    "try:\n",
    "    embedding_model = SentenceTransformer(CONFIG['EMBEDDING_MODEL_PATH'])\n",
    "    logger.info(f\"Successfully loaded SentenceTransformer model from: {CONFIG['EMBEDDING_MODEL_PATH']}\")\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Failed to load embedding model. Aborting. Error: {e}\", exc_info=True)\n",
    "    embedding_model = None # Ensure it's None if loading fails\n",
    "\n",
    "# --- 2. Load \"Hard Questions\" (Target Queries) ---\n",
    "if embedding_model:\n",
    "    hard_questions_list = []\n",
    "    hard_questions_ground_truths = []\n",
    "    try:\n",
    "        # Load the main dataset to get ground truths\n",
    "        main_ds = load_dataset('AI-MO/NuminaMath-CoT', split='train')\n",
    "        all_solutions_from_ds = main_ds['solution']\n",
    "        \n",
    "        # Load the hard questions JSON file\n",
    "        hard_questions_data = load_json(CONFIG['HARD_QUESTIONS_JSON_PATH'])\n",
    "        if hard_questions_data:\n",
    "            for index_str, question_text in hard_questions_data.items():\n",
    "                original_idx = int(index_str)\n",
    "                hard_questions_list.append(question_text)\n",
    "                hard_questions_ground_truths.append(all_solutions_from_ds[original_idx])\n",
    "            logger.info(f\"Successfully loaded {len(hard_questions_list)} hard questions and their ground truths.\")\n",
    "        else:\n",
    "            logger.critical(f\"Hard questions file not found or empty at {CONFIG['HARD_QUESTIONS_JSON_PATH']}. Aborting.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"An error occurred loading hard questions. Aborting. Error: {e}\", exc_info=True)\n",
    "\n",
    "# --- 3. Load Exemplar Corpus and Embeddings ---\n",
    "if embedding_model and hard_questions_list:\n",
    "    try:\n",
    "        corpus_ds = load_dataset(CONFIG['EXEMPLAR_CORPUS_NAME'], token=CONFIG.get('EXEMPLAR_CORPUS_HF_TOKEN'))\n",
    "        exemplar_questions = corpus_ds['train']['problem']\n",
    "        exemplar_solutions = corpus_ds['train']['solution']\n",
    "        \n",
    "        # Load embeddings (local -> Hugging Face -> generate)\n",
    "        local_path = CONFIG['EMBEDDED_EXEMPLAR_CORPUS_QUESTIONS_PATH']\n",
    "        if os.path.exists(local_path):\n",
    "            embedded_exemplars = np.load(local_path)\n",
    "            logger.info(f\"Loaded exemplar embeddings from local path: {local_path}\")\n",
    "        else:\n",
    "            logger.info(\"Local embeddings not found. Downloading from Hugging Face Hub.\")\n",
    "            hf_path = hf_hub_download(\n",
    "                repo_id=CONFIG['EXEMPLAR_EMBEDDINGS_HF_REPO_ID'],\n",
    "                filename=CONFIG['EXEMPLAR_EMBEDDINGS_HF_FILENAME'],\n",
    "                repo_type=\"dataset\"\n",
    "            )\n",
    "            embedded_exemplars = np.load(hf_path)\n",
    "            np.save(local_path, embedded_exemplars) # Save locally for next time\n",
    "            logger.info(f\"Downloaded and saved embeddings from {CONFIG['EXEMPLAR_EMBEDDINGS_HF_REPO_ID']}.\")\n",
    "            \n",
    "        exemplar_data = {\n",
    "            \"questions\": exemplar_questions,\n",
    "            \"solutions\": exemplar_solutions,\n",
    "            \"embeddings\": embedded_exemplars\n",
    "        }\n",
    "        logger.info(f\"Exemplar corpus loaded: {len(exemplar_questions)} questions with embeddings of shape {embedded_exemplars.shape}.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to load exemplar corpus or embeddings. Aborting. Error: {e}\", exc_info=True)\n",
    "        exemplar_data = None\n",
    "\n",
    "# --- 4. Initialize Gemini API Manager ---\n",
    "if embedding_model and hard_questions_list and exemplar_data:\n",
    "    try:\n",
    "        gemini_manager = GeminiAPIManager(\n",
    "            api_keys=CONFIG['GEMINI_API_KEYS'],\n",
    "            model_quotas=CONFIG['GEMINI_MODEL_QUOTAS'],\n",
    "            global_delay_seconds=CONFIG['GLOBAL_API_CALL_DELAY_SECONDS']\n",
    "        )\n",
    "        logger.info(\"GeminiAPIManager initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to initialize GeminiAPIManager. Aborting. Error: {e}\", exc_info=True)\n",
    "        gemini_manager = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the control panel for your research. Define different experiment\n",
    "# configurations by creating dictionaries that override the default CONFIG.\n",
    "\n",
    "experiment_configurations = [\n",
    "    {\n",
    "        \"experiment_name\": \"RAG_Only_Pass@3\",\n",
    "        \"APPLY_STANDARDIZATION\": False,\n",
    "        \"APPLY_TRANSFORMATION\": False,\n",
    "        \"APPLY_MERGING\": False,\n",
    "        \"TOP_N_CANDIDATES_RETRIEVAL\": 1,\n",
    "        \"N_PASS_ATTEMPTS\": 3,\n",
    "        \"DEFAULT_PASS_N_SOLVER_TEMPERATURE\": 1.0\n",
    "    },\n",
    "    {\n",
    "        \"experiment_name\": \"RAG_plus_Standardize_Pass@3\",\n",
    "        \"APPLY_STANDARDIZATION\": True,\n",
    "        \"APPLY_TRANSFORMATION\": False,\n",
    "        \"APPLY_MERGING\": False,\n",
    "        \"TOP_N_CANDIDATES_RETRIEVAL\": 1,\n",
    "        \"N_PASS_ATTEMPTS\": 3,\n",
    "        \"DEFAULT_PASS_N_SOLVER_TEMPERATURE\": 1.0,\n",
    "        \"DEFAULT_ADAPTATION_TEMPERATURE\": 0.0 # Be explicit about adaptation temp\n",
    "    },\n",
    "    {\n",
    "        \"experiment_name\": \"Full_Pipeline_K3_Pass@1\",\n",
    "        \"APPLY_STANDARDIZATION\": True,\n",
    "        \"APPLY_TRANSFORMATION\": True,\n",
    "        \"APPLY_MERGING\": True,\n",
    "        \"TOP_N_CANDIDATES_RETRIEVAL\": 3, # Retrieve more to allow for merging\n",
    "        \"TARGET_ADAPTED_SAMPLES_MERGING\": 1,\n",
    "        \"N_PASS_ATTEMPTS\": 1,\n",
    "        \"DEFAULT_PASS_N_SOLVER_TEMPERATURE\": 0.5 # Lower temp for single pass\n",
    "    },\n",
    "]\n",
    "\n",
    "logger.info(f\"Defined {len(experiment_configurations)} experiments to run.\")\n",
    "print(\"Experiments to run:\")\n",
    "for exp in experiment_configurations:\n",
    "    print(f\"- {exp['experiment_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084fb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes the logs generated by the experiments and runs the\n",
    "# LLM-based evaluation to produce the final Pass@K summary.\n",
    "\n",
    "# --- NEW: Import for final sync ---\n",
    "from src.hf_sync import sync_workspace_to_hub\n",
    "\n",
    "if all_experiment_logs:\n",
    "    logger.info(\"Starting analysis of experiment results.\")\n",
    "    \n",
    "    # This single function call evaluates all experiments and returns a DataFrame\n",
    "    summary_df = analyze_experiment_logs(\n",
    "        all_experiments_logs=all_experiment_logs,\n",
    "        ground_truths=hard_questions_ground_truths,\n",
    "        gemini_manager=gemini_manager,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    # Save the final summary to a CSV file for easy access\n",
    "    summary_path = os.path.join(CONFIG['RESULTS_DIR'], \"final_experiment_summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    \n",
    "    logger.info(f\"Analysis complete. Summary saved to {summary_path}\")\n",
    "    \n",
    "    # Display the final results in the notebook\n",
    "    print(\"\\n--- Experiment Summary ---\")\n",
    "    \n",
    "    # Set display options for better viewing of the DataFrame\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    \n",
    "    display(summary_df)\n",
    "\n",
    "else:\n",
    "    logger.warning(\"No experiment logs were generated or loaded. Skipping analysis.\")\n",
    "\n",
    "# --- NEW: Final synchronization to save all results ---\n",
    "logger.info(\"Performing final synchronization to Hugging Face Hub.\")\n",
    "sync_workspace_to_hub(CONFIG)\n",
    "logger.info(\"Final sync complete. All results are saved online.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes the logs generated by the experiments and runs the\n",
    "# LLM-based evaluation to produce the final Pass@K summary.\n",
    "\n",
    "# --- NEW: Import for final sync ---\n",
    "from src.hf_sync import sync_workspace_to_hub\n",
    "\n",
    "if all_experiment_logs:\n",
    "    logger.info(\"Starting analysis of experiment results.\")\n",
    "    \n",
    "    # This single function call evaluates all experiments and returns a DataFrame\n",
    "    summary_df = analyze_experiment_logs(\n",
    "        all_experiments_logs=all_experiment_logs,\n",
    "        ground_truths=hard_questions_ground_truths,\n",
    "        gemini_manager=gemini_manager,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    # Save the final summary to a CSV file for easy access\n",
    "    summary_path = os.path.join(CONFIG['RESULTS_DIR'], \"final_experiment_summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    \n",
    "    logger.info(f\"Analysis complete. Summary saved to {summary_path}\")\n",
    "    \n",
    "    # Display the final results in the notebook\n",
    "    print(\"\\n--- Experiment Summary ---\")\n",
    "    \n",
    "    # Set display options for better viewing of the DataFrame\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    \n",
    "    display(summary_df)\n",
    "\n",
    "else:\n",
    "    logger.warning(\"No experiment logs were generated or loaded. Skipping analysis.\")\n",
    "\n",
    "# --- NEW: Final synchronization to save all results ---\n",
    "logger.info(\"Performing final synchronization to Hugging Face Hub.\")\n",
    "sync_workspace_to_hub(CONFIG)\n",
    "logger.info(\"Final sync complete. All results are saved online.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

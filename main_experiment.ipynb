{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e86597",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install datasets\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download, HfFolder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Our project's modules ---\n",
    "from config import CONFIG, setup_directories\n",
    "from src.utils import setup_logger, load_json\n",
    "from src.api_manager import GeminiAPIManager\n",
    "from src.orchestration import run_experiments\n",
    "from src.evaluation import analyze_experiment_logs\n",
    "# --- NEW: Import for HF synchronization ---\n",
    "from src.hf_sync import initialize_workspace, sync_workspace_to_hub\n",
    "\n",
    "# --- Initial Setup ---\n",
    "# 1. Create the directory structure (logs, results, etc.)\n",
    "setup_directories()\n",
    "\n",
    "# 2. Initialize the main logger for the notebook\n",
    "logger = setup_logger('main_orchestrator', log_dir=CONFIG['LOGS_DIR'])\n",
    "\n",
    "# --- MODIFIED: Hugging Face Hub Workspace Initialization from Config ---\n",
    "if CONFIG.get('PERSIST_RESULTS_ONLINE'):\n",
    "    # The token is now read directly from the CONFIG dictionary.\n",
    "    # Make sure you have set it in your config.py file.\n",
    "    if CONFIG.get(\"HF_SYNC_TOKEN\"):\n",
    "        logger.info(\"HF_SYNC_TOKEN found in config. Initializing workspace from Hugging Face Hub.\")\n",
    "        initialize_workspace(CONFIG)\n",
    "    else:\n",
    "        logger.warning(\"PERSIST_RESULTS_ONLINE is True, but no HF_SYNC_TOKEN was found in config.py.\")\n",
    "        logger.warning(\"Online persistence will fail. Please add your token to config.py to enable it.\")\n",
    "\n",
    "logger.info(\"Notebook execution started. Directories and logger are set up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell handles loading all necessary assets. If any part fails,\n",
    "# the execution should stop here.\n",
    "\n",
    "# --- 1. Load Embedding Model ---\n",
    "try:\n",
    "    embedding_model = SentenceTransformer(CONFIG['EMBEDDING_MODEL_PATH'])\n",
    "    logger.info(f\"Successfully loaded SentenceTransformer model from: {CONFIG['EMBEDDING_MODEL_PATH']}\")\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Failed to load embedding model. Aborting. Error: {e}\", exc_info=True)\n",
    "    embedding_model = None # Ensure it's None if loading fails\n",
    "\n",
    "# --- 2. Load \"Hard Questions\" (Target Queries) using Indices ---\n",
    "if embedding_model:\n",
    "    hard_questions_list = []\n",
    "    hard_questions_ground_truths = []\n",
    "    try:\n",
    "        # Load the main dataset which now serves as the source for all questions\n",
    "        main_ds = load_dataset('AI-MO/NuminaMath-CoT', split='train')\n",
    "        \n",
    "        # Load the indices that specify which questions are \"hard\"\n",
    "        # This uses the new path from your updated config.py\n",
    "        hard_question_indices = load_json(CONFIG['HARD_QUESTIONS_INDICES_PATH'])\n",
    "        \n",
    "        if hard_question_indices:\n",
    "            # Use the indices to directly select the questions and solutions from the main dataset\n",
    "            hard_questions_list = [main_ds['problem'][i] for i in hard_question_indices]\n",
    "            hard_questions_ground_truths = [main_ds['solution'][i] for i in hard_question_indices]\n",
    "            logger.info(f\"Successfully loaded {len(hard_questions_list)} hard questions and their ground truths using indices.\")\n",
    "        else:\n",
    "            logger.critical(f\"Hard questions index file not found or empty at {CONFIG['HARD_QUESTIONS_INDICES_PATH']}. Aborting.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"An error occurred loading hard questions. Aborting. Error: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- 2. Load \"Hard Questions\" (Target Queries) ---\n",
    "if embedding_model:\n",
    "    hard_questions_list = []\n",
    "    hard_questions_ground_truths = []\n",
    "    try:\n",
    "        # Load the main dataset\n",
    "        main_ds = load_dataset('AI-MO/NuminaMath-CoT', split='train')\n",
    "\n",
    "        # --- NEW LOGIC ---\n",
    "        # Load the indices of the hard questions\n",
    "        hard_question_indices = load_json(CONFIG['HARD_QUESTIONS_INDICES_PATH'])\n",
    "\n",
    "        if hard_question_indices:\n",
    "            # Directly create the lists using the indices\n",
    "            hard_questions_list = [main_ds['problem'][i] for i in hard_question_indices]\n",
    "            hard_questions_ground_truths = [main_ds['solution'][i] for i in hard_question_indices]\n",
    "\n",
    "            # Update the log message\n",
    "            logger.info(f\"Successfully loaded {len(hard_questions_list)} hard questions and their ground truths using indices.\")\n",
    "        else:\n",
    "            logger.critical(f\"Hard questions index file not found or empty at {CONFIG['HARD_QUESTIONS_INDICES_PATH']}. Aborting.\")\n",
    "        # --- END NEW LOGIC ---\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"An error occurred loading hard questions. Aborting. Error: {e}\", exc_info=True)\n",
    "        \n",
    "# --- 4. Initialize Gemini API Manager ---\n",
    "if embedding_model and hard_questions_list and exemplar_data:\n",
    "    try:\n",
    "        gemini_manager = GeminiAPIManager(\n",
    "            api_keys=CONFIG['GEMINI_API_KEYS'],\n",
    "            model_quotas=CONFIG['GEMINI_MODEL_QUOTAS'],\n",
    "            global_delay_seconds=CONFIG['GLOBAL_API_CALL_DELAY_SECONDS']\n",
    "        )\n",
    "        logger.info(\"GeminiAPIManager initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to initialize GeminiAPIManager. Aborting. Error: {e}\", exc_info=True)\n",
    "        gemini_manager = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the control panel for your research. Define different experiment\n",
    "# configurations by creating dictionaries that override the default CONFIG.\n",
    "\n",
    "experiment_configurations = [\n",
    "    {\n",
    "        \"experiment_name\": \"RAG_Only_Pass@3\",\n",
    "        \"USE_RETRIEVAL\": True, # Explicitly set for clarity\n",
    "        \"APPLY_STANDARDIZATION\": False,\n",
    "        \"APPLY_TRANSFORMATION\": False,\n",
    "        \"APPLY_MERGING\": False,\n",
    "        \"TOP_N_CANDIDATES_RETRIEVAL\": 1,\n",
    "        \"N_PASS_ATTEMPTS\": 3,\n",
    "        \"DEFAULT_PASS_N_SOLVER_TEMPERATURE\": 1.0\n",
    "    },\n",
    "    {\n",
    "        \"experiment_name\": \"RAG_plus_Standardize_Pass@3\",\n",
    "        \"APPLY_STANDARDIZATION\": True,\n",
    "        \"APPLY_TRANSFORMATION\": False,\n",
    "        \"APPLY_MERGING\": False,\n",
    "        \"TOP_N_CANDIDATES_RETRIEVAL\": 1,\n",
    "        \"N_PASS_ATTEMPTS\": 3,\n",
    "        \"DEFAULT_PASS_N_SOLVER_TEMPERATURE\": 1.0,\n",
    "        \"DEFAULT_ADAPTATION_TEMPERATURE\": 0.0 # Be explicit about adaptation temp\n",
    "    },\n",
    "    {\n",
    "        \"experiment_name\": \"Full_Pipeline_K3_Pass@1\",\n",
    "        \"APPLY_STANDARDIZATION\": True,\n",
    "        \"APPLY_TRANSFORMATION\": True,\n",
    "        \"APPLY_MERGING\": True,\n",
    "        \"TOP_N_CANDIDATES_RETRIEVAL\": 3, # Retrieve more to allow for merging\n",
    "        \"TARGET_ADAPTED_SAMPLES_MERGING\": 1,\n",
    "        \"N_PASS_ATTEMPTS\": 1,\n",
    "        \"DEFAULT_PASS_N_SOLVER_TEMPERATURE\": 0.5 # Lower temp for single pass\n",
    "    },\n",
    "]\n",
    "\n",
    "logger.info(f\"Defined {len(experiment_configurations)} experiments to run.\")\n",
    "print(\"Experiments to run:\")\n",
    "for exp in experiment_configurations:\n",
    "    print(f\"- {exp['experiment_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084fb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes the logs generated by the experiments and runs the\n",
    "# LLM-based evaluation to produce the final Pass@K summary.\n",
    "\n",
    "if all_experiment_logs:\n",
    "    logger.info(\"Starting analysis of experiment results.\")\n",
    "    \n",
    "    # This single function call evaluates all experiments and returns a DataFrame\n",
    "    summary_df = analyze_experiment_logs(\n",
    "        all_experiments_logs=all_experiment_logs,\n",
    "        ground_truths=hard_questions_ground_truths,\n",
    "        gemini_manager=gemini_manager,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    # Save the final summary to a CSV file for easy access\n",
    "    summary_path = os.path.join(CONFIG['RESULTS_DIR'], \"final_experiment_summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    \n",
    "    logger.info(f\"Analysis complete. Summary saved to {summary_path}\")\n",
    "    \n",
    "    # Display the final results in the notebook\n",
    "    print(\"\\n--- Experiment Summary ---\")\n",
    "    \n",
    "    # Set display options for better viewing of the DataFrame\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    \n",
    "    display(summary_df)\n",
    "\n",
    "else:\n",
    "    logger.warning(\"No experiment logs were generated or loaded. Skipping analysis.\")\n",
    "\n",
    "# --- NEW: Final synchronization to save all results ---\n",
    "logger.info(\"Performing final synchronization to Hugging Face Hub.\")\n",
    "sync_workspace_to_hub(CONFIG)\n",
    "logger.info(\"Final sync complete. All results are saved online.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac005a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes the logs generated by the experiments and runs the\n",
    "# LLM-based evaluation to produce the final Pass@K summary.\n",
    "\n",
    "# --- NEW: Import for final sync ---\n",
    "from src.hf_sync import sync_workspace_to_hub\n",
    "\n",
    "if all_experiment_logs:\n",
    "    logger.info(\"Starting analysis of experiment results.\")\n",
    "    \n",
    "    # This single function call evaluates all experiments and returns a DataFrame\n",
    "    summary_df = analyze_experiment_logs(\n",
    "        all_experiments_logs=all_experiment_logs,\n",
    "        ground_truths=hard_questions_ground_truths,\n",
    "        gemini_manager=gemini_manager,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    # Save the final summary to a CSV file for easy access\n",
    "    summary_path = os.path.join(CONFIG['RESULTS_DIR'], \"final_experiment_summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    \n",
    "    logger.info(f\"Analysis complete. Summary saved to {summary_path}\")\n",
    "    \n",
    "    # Display the final results in the notebook\n",
    "    print(\"\\n--- Experiment Summary ---\")\n",
    "    \n",
    "    # Set display options for better viewing of the DataFrame\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    \n",
    "    display(summary_df)\n",
    "\n",
    "else:\n",
    "    logger.warning(\"No experiment logs were generated or loaded. Skipping analysis.\")\n",
    "\n",
    "# --- NEW: Final synchronization to save all results ---\n",
    "logger.info(\"Performing final synchronization to Hugging Face Hub.\")\n",
    "sync_workspace_to_hub(CONFIG)\n",
    "logger.info(\"Final sync complete. All results are saved online.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21075dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW CELL: RETRY FAILED EVALUATIONS ---\n",
    "# This cell attempts to retry the evaluations that failed due to API errors\n",
    "# and then recalculates the summary to see the impact of the retries.\n",
    "\n",
    "# --- Import the new error handler functions ---\n",
    "from src.error_handler import retry_failed_evaluations\n",
    "\n",
    "if all_experiment_logs:\n",
    "    logger.info(\"Starting the retry process for failed evaluations.\")\n",
    "    \n",
    "    # This function will find API_ERRORs in the logs and retry them, updating the logs in place.\n",
    "    retry_failed_evaluations(\n",
    "        all_experiments_logs=all_experiment_logs,\n",
    "        ground_truths=hard_questions_ground_truths,\n",
    "        gemini_manager=gemini_manager,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Retry process complete. Recalculating the experiment summary.\")\n",
    "    \n",
    "    # Now, we run the analysis again to get the updated summary\n",
    "    updated_summary_df = analyze_experiment_logs(\n",
    "        all_experiments_logs=all_experiment_logs,\n",
    "        ground_truths=hard_questions_ground_truths,\n",
    "        gemini_manager=gemini_manager,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    # Save the new summary to a different CSV file\n",
    "    updated_summary_path = os.path.join(CONFIG['RESULTS_DIR'], \"final_experiment_summary_after_retry.csv\")\n",
    "    updated_summary_df.to_csv(updated_summary_path, index=False)\n",
    "    \n",
    "    logger.info(f\"Analysis after retry complete. New summary saved to {updated_summary_path}\")\n",
    "    \n",
    "    # Display the new summary\n",
    "    print(\"\\n--- Experiment Summary After Retries ---\")\n",
    "    display(updated_summary_df)\n",
    "\n",
    "else:\n",
    "    logger.warning(\"No experiment logs were loaded. Skipping retry process.\")\n",
    "\n",
    "# --- Final sync to save the updated logs and summary ---\n",
    "logger.info(\"Performing final synchronization after retries.\")\n",
    "sync_workspace_to_hub(CONFIG)\n",
    "logger.info(\"Final sync complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a572932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW CELL: API ERROR EXPLORATION REPORT ---\n",
    "# This cell generates a detailed report of all API errors that occurred\n",
    "# during the evaluation process, helping to diagnose the issues.\n",
    "\n",
    "# --- Import the error reporting function ---\n",
    "from src.error_handler import generate_error_report\n",
    "\n",
    "if all_experiment_logs:\n",
    "    logger.info(\"Generating a detailed report of API errors from the evaluation logs.\")\n",
    "    \n",
    "    # This function scans all logs and compiles a report of API_ERRORs\n",
    "    error_df = generate_error_report(\n",
    "        all_experiments_logs=all_experiment_logs,\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    if not error_df.empty:\n",
    "        print(\"\\n--- API Error Exploration Report ---\")\n",
    "        # Set display options for better viewing of the DataFrame\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', 1000)\n",
    "        pd.set_option('display.max_rows', 100)\n",
    "        \n",
    "        display(error_df)\n",
    "        \n",
    "        # Save the report to a CSV file for further analysis\n",
    "        error_report_path = os.path.join(CONFIG['RESULTS_DIR'], \"api_error_report.csv\")\n",
    "        error_df.to_csv(error_report_path, index=False)\n",
    "        logger.info(f\"API error report saved to {error_report_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n--- No API Errors Found ---\")\n",
    "        logger.info(\"The error report is empty because no API_ERRORs were logged.\")\n",
    "else:\n",
    "    logger.warning(\"No experiment logs loaded, cannot generate an error report.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Check API Keys ---\n",
    "from src.utils import check_api_keys\n",
    "\n",
    "if gemini_manager:\n",
    "    check_api_keys(CONFIG)\n",
    "else:\n",
    "    logger.warning(\"Skipping API key check because GeminiAPIManager failed to initialize.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

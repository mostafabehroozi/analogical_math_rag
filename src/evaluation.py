# src/evaluation.py

"""
Evaluation module for the Analogical Reasoning RAG project.

This file provides the core logic for assessing the performance of the generated
solutions. Its responsibilities include:

1.  Evaluating a single generated answer against a ground truth using an LLM.
2.  Parsing and analyzing the detailed JSON logs from experiment runs to manage
    state and resume progress.
3.  Calculating Pass@K metrics and aggregating error statistics across all
    experiments.
4.  Generating a final summary DataFrame for analysis and reporting.

This rewritten version ensures robustness, clarity, and includes the critical
fix to prevent notebook crashes, allowing the retry logic to function correctly.
"""

import logging
import os
import re
import pandas as pd
from tqdm import tqdm
from typing import List, Dict, Any, Tuple, Optional, TypedDict

# Import custom project modules
from src.prompts import create_evaluation_prompt
from src.utils import save_json, load_json
from src.hf_sync import periodic_sync_check
# MODIFIED: Import manager classes for type checking and APIResponse type
from src.api_manager import APIResponse, GeminiAPIManager, AvalAIAPIManager, OllamaAPIManager

# Define a structured type for the result of a single LLM-based evaluation
# This improves clarity over a simple Tuple.
class EvaluationResult(TypedDict):
    """The structured result of a single answer evaluation call."""
    is_correct: Optional[bool]
    status: str
    error_details: Optional[APIResponse]


def evaluate_single_answer_with_llm(
    model_answer: str,
    ground_truth: str,
    api_manager: Any,
    config: Dict[str, Any]
) -> EvaluationResult:
    """
    Evaluates a single model-generated answer against a ground truth using an LLM.

    This function constructs a prompt for an evaluator LLM, calls the API,
    and parses the response to determine if the model's answer is correct.
    It is designed to handle API errors and parsing failures gracefully.

    Args:
        model_answer (str): The solution text generated by the pipeline.
        ground_truth (str): The correct solution text.
        api_manager (Any): The instantiated API manager (Gemini or AvalAI).
        config (Dict[str, Any]): The global configuration dictionary.

    Returns:
        EvaluationResult: A dictionary containing the correctness (bool), a status
                          string, and any error details from the API.
    """
    logger = logging.getLogger(__name__)

    # Guard clause: An empty or invalid answer is a distinct failure type.
    if not model_answer or not isinstance(model_answer, str):
        return {"is_correct": None, "status": "EMPTY_ANSWER", "error_details": None}

    # MODIFIED: Determine which LLM model to use based on the type of the active manager
    if isinstance(api_manager, GeminiAPIManager):
        evaluator_model = config['GEMINI_MODEL_NAME_EVALUATOR']
    elif isinstance(api_manager, AvalAIAPIManager):
        evaluator_model = config['AVALAI_MODEL_NAME_EVALUATOR']
    elif isinstance(api_manager, OllamaAPIManager):
        evaluator_model = config['OLLAMA_MODEL_NAME_EVALUATOR']
    else:
        raise TypeError(f"Unsupported API manager type for evaluation: {type(api_manager)}")
        
    evaluator_temp = config['DEFAULT_EVALUATOR_TEMPERATURE']

    prompt = create_evaluation_prompt(model_answer, ground_truth, config)
    
    print(f"      [API Context] Calling LLM for: Evaluation")
    response = api_manager.generate_content(prompt, evaluator_model, evaluator_temp)

    # Handle API call failures
    if response['status'] != 'SUCCESS':
        error_msg = response.get('error_message', 'Unknown API failure')
        logger.warning(f"LLM evaluation API call failed with status '{response['status']}': {error_msg}")
        return {"is_correct": None, "status": f"API_{response['status']}", "error_details": response}

    # Handle successful API call, but potentially malformed response
    raw_text = response.get('text', '').strip()
    trunc_len = config.get("API_RESPONSE_TRUNCATION_LENGTH", 50)
    print(f"      Evaluator LLM Raw Output (truncated): {raw_text[:trunc_len]}{'...' if len(raw_text) > trunc_len else ''}") 
    logger.debug(f"Evaluator raw response: '{raw_text}'")

    # Parse the 'Evaluation: [true/false]' line from the response
    eval_match = re.search(r"Evaluation:\s*(true|false)", raw_text, re.IGNORECASE)
    if eval_match:
        result_str = eval_match.group(1).lower()
        logger.info(f"Parsed evaluation result: {result_str}")
        return {"is_correct": result_str == 'true', "status": "SUCCESS", "error_details": None}
    else:
        logger.warning(f"Could not parse 'Evaluation:' line from LLM response. Treating as parsing failure.")
        return {"is_correct": None, "status": "PARSING_FAILED", "error_details": None}


def analyze_experiment_logs(
    all_experiments_logs: Dict[str, List[Dict]],
    ground_truths: List[str],
    api_managers: Dict[str, Any],
    config: Dict[str, Any]
) -> pd.DataFrame:
    """
    Analyzes all experiment logs, performs evaluations, and calculates Pass@K metrics.

    This function orchestrates the end-to-end analysis process. It loads any
    previously saved evaluation results to avoid re-running work. For any new
    (unevaluated) generation logs, it calls the LLM evaluator. Finally, it
    aggregates all results to compute Pass@K scores and error statistics for
    each experiment.

    Args:
        all_experiments_logs (Dict): A dictionary mapping experiment names to their run logs.
        ground_truths (List[str]): A list of ground truth solutions, indexed by `hard_list_idx`.
        api_managers (Dict[str, Any]): The dictionary of instantiated API managers.
        config (Dict[str, Any]): The global configuration dictionary.

    Returns:
        pd.DataFrame: A DataFrame summarizing the results, with one row per experiment,
                      containing Pass@K scores and error counts.
    """
    logger = logging.getLogger(__name__)
    logger.info("Starting comprehensive analysis of all experiment logs.")
    analysis_summary = []

    pass_k_values_to_report = config.get("PASS_K_VALUES_TO_REPORT", [1])
    results_dir = config['RESULTS_DIR']

    for exp_name, query_logs in all_experiments_logs.items():
        logger.info(f"--- Analyzing Experiment: {exp_name} ---")
        if not query_logs:
            logger.warning(f"No logs found for experiment '{exp_name}'. Skipping.")
            continue

        exp_config = query_logs[0].get("config_flags_used", {})
        n_attempts_config = exp_config.get("N_PASS_ATTEMPTS", 1)
        pass_k_values = sorted([k for k in pass_k_values_to_report if 0 < k <= n_attempts_config])

        # --- Phase 1: State Management & Resumption ---
        eval_file_path = os.path.join(results_dir, f"{exp_name}_detailed_eval.json")
        detailed_evaluations = load_json(eval_file_path) or []
        evaluated_indices = {eval_log['hard_list_idx'] for eval_log in detailed_evaluations}
        logger.info(f"Loaded {len(detailed_evaluations)} existing evaluation results for '{exp_name}'.")

        # --- Phase 2: Process New (Unevaluated) Logs ---
        logs_to_process = [log for log in query_logs if log["target_query_original_hard_list_idx"] not in evaluated_indices]

        if not logs_to_process:
            logger.info(f"All query logs for '{exp_name}' have already been evaluated. Proceeding to summary.")
        else:
            logger.info(f"Found {len(logs_to_process)} new query logs to evaluate for '{exp_name}'.")

            # --- MODIFIED: Select the correct API manager for evaluation ---
            provider_for_eval = config.get('API_PROVIDER_EVALUATOR', 'gemini')
            manager_for_eval = api_managers[provider_for_eval]
            
            for loop_idx, log in enumerate(tqdm(logs_to_process, desc=f"Evaluating {exp_name}")):
                hard_list_idx = log["target_query_original_hard_list_idx"]
                ground_truth = ground_truths[hard_list_idx]
                
                solution_attempts = log.get("steps", {}).get("solving", {}).get("solution_attempts", [])
                
                is_correct_list, status_list, error_details_list = [], [], []

                for i, attempt in enumerate(solution_attempts):
                    # Case 1: Generation was successful (attempt is a string)
                    if isinstance(attempt, str):
                        print(f"    -> Evaluating attempt {i+1}/{len(solution_attempts)} for query #{hard_list_idx}")
                        eval_result = evaluate_single_answer_with_llm(attempt, ground_truth, manager_for_eval, config)
                        if eval_result["status"] != "SUCCESS":
                            print(f"       WARNING: Evaluation attempt failed with status: {eval_result['status']}")
                        
                        is_correct_list.append(eval_result["is_correct"])
                        status_list.append(eval_result["status"])
                        error_details_list.append(eval_result["error_details"])
                    
                    # Case 2: Generation failed (attempt is a dictionary with error info)
                    elif isinstance(attempt, dict) and attempt.get('status') == 'FAILURE':
                        print(f"    -> Skipping evaluation for attempt {i+1} (generation failed).")
                        is_correct_list.append(None)
                        status_list.append("GENERATION_FAILED")
                        error_details_list.append(attempt.get("error_info"))

                    # Case 3: Unexpected format
                    else:
                        is_correct_list.append(None)
                        status_list.append("INVALID_ATTEMPT_FORMAT")
                        error_details_list.append(None)

                detailed_evaluations.append({
                    "hard_list_idx": hard_list_idx,
                    "is_correct_list": is_correct_list,
                    "evaluation_status_list": status_list,
                    "evaluation_error_details": error_details_list,
                    "attempts": solution_attempts  # Store original attempts for context
                })

                # Persist progress incrementally
                save_json(detailed_evaluations, eval_file_path)
                periodic_sync_check(loop_idx, config)

        # Final save after processing all new logs for this experiment
        save_json(detailed_evaluations, eval_file_path)

        # --- Phase 3: Aggregate Results from All Evaluations (Old and New) ---
        pass_k_counts = {k: 0 for k in pass_k_values}
        error_counts = {"API_BLOCKED": 0, "API_ERROR": 0, "API_RATE_LIMITED": 0, "PARSING_FAILED": 0, "EMPTY_ANSWER": 0, "GENERATION_FAILED": 0}
        total_valid_queries = 0

        for eval_result in detailed_evaluations:
            # Only count queries where there was at least one generation attempt
            if eval_result.get("attempts"):
                total_valid_queries += 1
                is_correct_list = eval_result["is_correct_list"]
                status_list = eval_result["evaluation_status_list"]

                # Tally all evaluation statuses/errors
                for status in status_list:
                    error_key = status.replace("API_", "")
                    if error_key in error_counts:
                        error_counts[error_key] += 1
                    elif status in error_counts: # Handles non-API statuses
                        error_counts[status] += 1

                # Calculate Pass@K
                for k in pass_k_values:
                    if any(is_correct_list[:k]):
                        pass_k_counts[k] += 1
        
        # --- Phase 4: Compile and Store the Final Summary for this Experiment ---
        exp_summary = {
            "experiment_name": exp_name,
            "num_questions_evaluated": total_valid_queries,
            **{f"pass@{k}": (pass_k_counts[k] / total_valid_queries if total_valid_queries > 0 else 0) for k in pass_k_values},
            **{f"errors_{key.lower()}": count for key, count in error_counts.items()}
        }
        analysis_summary.append(exp_summary)
        
    # After processing all experiments, create and return the final DataFrame.
    # This `return` statement is the critical fix.
    logger.info("Finished analysis of all experiments.")
    return pd.DataFrame(analysis_summary)